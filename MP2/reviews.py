# -*- coding: utf-8 -*-
"""reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iJNYbYhvOX65w8pF2yRdVXKIj1TMGTge

# Introduction

**2nd mini-project**

Natural Language

Authors: Piotr MigdaÅ‚ek, Jascha Jestel

Group: 74

Task: Hotel reviews - Sentiment analysis

# Load Data
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install GitPython
# %pip install wordcloud
# %pip install seaborn
# %pip install nltk
# %pip install transformers
# %pip install lightgbm
# %pip install datasets

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

import git
import os
import re

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional
from keras.preprocessing.text import Tokenizer

from lightgbm import LGBMClassifier

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

from transformers import AutoTokenizer, TFAutoModel, TFDistilBertForSequenceClassification, TFDistilBertModel
from datasets import Dataset

from numpy.random import seed
seed(42)

DATA_SOURCE = 'DRIVE'

if DATA_SOURCE == 'GIT':
  repo = git.Repo('.', search_parent_directories=True)
  data = train = pd.read_csv(os.path.join(repo.working_tree_dir, "MP2\\train.txt"), sep='\t', header = 0, names = ['sentiment','review'])

elif DATA_SOURCE == 'DRIVE':
  from google.colab import drive
  drive.mount('/content/drive')

  data = pd.read_csv("/content/drive/MyDrive/train.txt", sep='\t', header = 0, names = ['sentiment','review'])

data.head(10)

"""# No preprocessing LSTM baseline"""

x_train, x_test, y_train, y_test = train_test_split(
    data["review"].values,
    data["sentiment"].values,
    test_size=0.2,
    random_state=42,
    stratify=data["sentiment"].values
)

lb = LabelBinarizer()
y_train = lb.fit_transform(y_train)
y_test = lb.fit(y_test)

print(x_train.shape)
print(x_test.shape)

#Tokenizing text
max_vocab = 50000
tokenizer = Tokenizer(num_words = max_vocab)
tokenizer.fit_on_texts(x_train)

#Turning text into sequence
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)

#Padding sequences
x_train_seq_pad = pad_sequences(x_train_seq)
x_test_seq_pad = pad_sequences(x_test_seq, maxlen = np.array(x_train_seq_pad).shape[1])

"""## Embedding (100-dim.) + LSTM

"""

model = keras.Sequential(
    [
        Embedding(max_vocab + 1, 100, input_length = np.array(x_train_seq_pad).shape[1], mask_zero=True),
        LSTM(128, dropout = 0.3, recurrent_dropout = 0.3),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(4, activation='softmax')
    ]
)
model.summary()

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
model.fit(x_train_seq_pad, y_train, batch_size = 64, epochs = 8, validation_split=0.1)

score = model.evaluate(x_test_seq_pad, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""## Embedding (64-dim.) + LSTM"""

model2 = keras.Sequential(
    [
        Embedding(max_vocab + 1, 64, input_length = np.array(x_train_seq_pad).shape[1]),
        LSTM(128, dropout = 0.3, recurrent_dropout = 0.3),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(4, activation='softmax')
    ]
)
model2.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
model2.fit(x_train_seq_pad, y_train, batch_size = 32, epochs = 8, validation_split=0.1)
score = model2.evaluate(x_test_seq_pad, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""## Embedding (128-dim.) + Bidirectional LSTM"""

model3 = keras.Sequential(
    [
        Embedding(max_vocab + 1, 128, input_length = np.array(x_train_seq_pad).shape[1]),
        Bidirectional(LSTM(64, return_sequences=True)),
        Bidirectional(LSTM(64)),
        Dense(4, activation='softmax')
    ]
)
model3.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
model3.fit(x_train_seq_pad, y_train, batch_size = 32, epochs = 5, validation_split=0.1)
score = model3.evaluate(x_test_seq_pad, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

"""Due to the corpus having not sufficent (~1400) number of reviews, neural embedding was probably fitted poorly and then LSTM network performed fairly mediocore (~70%) due to overfitting tendency.

# EDA
"""

plt.figure(figsize=(6, 6))
labels = ['TRUTHFULPOSITIVE', 'TRUTHFULNEGATIVE', 'DECEPTIVEPOSITIVE', 'DECEPTIVENEGATIVE']
values = [len(data[data['sentiment'] == label]) for label in labels]
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Class Distribution Pie Chart')
plt.axis('equal')
plt.show()

g = sns.histplot(data['review'].apply(lambda x: len(x.split())))
g.set(xlabel='Number of words in review',
      ylabel='Number of reviews',
      title='Review word-count distribution')
plt.show()

def print_word_cloud(label):
    wcTP = WordCloud(max_words=1000,
                     min_font_size=10,
                     height=400,
                     width=800,
                     background_color="black").generate(" ".join(data[data["sentiment"]==label].review))
    fig, ax = plt.subplots(figsize=(8,8))
    ax.imshow(wcTP, interpolation='bilinear')
    ax.set_axis_off()
    ax.set_title('Word Cloud for TRUTHFULPOSITIVE reviews')
    plt.show()

print_word_cloud("TRUTHFULPOSITIVE")
print_word_cloud("TRUTHFULNEGATIVE")
print_word_cloud("DECEPTIVEPOSITIVE")
print_word_cloud("DECEPTIVENEGATIVE")

data['length'] = data['review'].apply(lambda x: len(x))

plt.figure(figsize=(8, 4))
sns.kdeplot(data=data, x='length', hue='sentiment', common_norm=False, fill=True)
plt.xlabel('Review Length (number of characters))')
plt.ylabel('Density')
plt.title('Density plot of review lengths by sentiment')
plt.legend(title='Sentiment', labels=data['sentiment'].unique())
plt.show()

"""Let's look at all caps words usage. Maybe more in negative reviews?"""

def count_all_caps(text):
    return len(re.findall(r'\b[A-Z]{2,}\b', text))

data["all_caps_count"] = data['review'].apply(count_all_caps)
# data['positive'] = (data['sentiment'] == 'TRUTHFULPOSITIVE') | (data['sentiment'] == 'DECEPTIVEPOSITIVE')
# data['truthful'] = (data['sentiment'] == 'DECEPTIVENEGATIVE') | (data['sentiment'] == 'DECEPTIVEPOSITIVE')

sns.histplot(data=data[data['all_caps_count'] > 0] , x='all_caps_count', hue='sentiment', multiple='stack', discrete=True)
plt.xlabel("# of all caps words in review")
plt.show()

"""Mostly no all caps words used. Some include a few. Some outliers include many. Let's look at the sentiment proportions. Therefore, use 3 or more as one group for the number fo all caps words in a review because of low support for more than that."""

data.loc[data["all_caps_count"] >= 3, "all_caps_count"] = '3 or more'
pd.crosstab(index=data["all_caps_count"], columns=data["sentiment"])

cross_tab = pd.crosstab(index=data["all_caps_count"], columns=data["sentiment"], normalize="index")
cross_tab.plot(kind="bar", stacked=True, figsize=(8, 4))
plt.ylabel("Proportion")
plt.xlabel("# of all caps words in review")
plt.show()

"""Looks like proportion of TRUSTFULNEGATIVE is increasing with the # of all caps words in a review.

Since the number of all caps words used isn't that great, we can just preserve them in the preprocessing.

# Preprocessing

## Lemmatization + Stop Word Removal
"""

# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('omw-1.4')
# nltk.download('wordnet')

data['review'][0]

# Lemmatize with POS Tag (Parts of Speech tagging)
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)


def clean(review):
    review = re.sub("[^a-z A-Z 0-9-]+", "", review)

    # tokenize the sentences
    review = word_tokenize(review)

    # transform to lower case if not all caps
    review = [w.lower() if not w.isupper() else w for w in review]

    # filter stopwords
    review = [t for t in review if t not in stopwords.words("english")]

    # lemmatize each token
    review = [lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in review]

    return review


lemmatizer = WordNetLemmatizer()
data["review_clean"] = data["review"].apply(clean)

"""## Soft cleaning"""

# only convert to lover case and remove double spaces for DistilBERT model
def soft_clean_text(text):
    text = text.lower()
    # text = re.sub("[^a-z A-Z]+", "", text) # decide if only a-z...
    text = re.sub(r'\s+', ' ', text)
    return text


data['review_soft_clean'] = data["review"].apply(soft_clean_text)

"""# Model Training

## TF-IDF
"""

# train (80%) and test split (20%)
# save index for train and test set to use for all models
train_index, test_index, _, _ = train_test_split(
    range(len(data)),
    data["sentiment"].values,
    test_size=0.2,
    random_state=42,
    stratify=data["sentiment"].values
)

x_train = data["review_clean"].values[train_index]
x_test = data["review_clean"].values[test_index]
y_train = data["sentiment"].values[train_index]
y_test = data["sentiment"].values[test_index]

def id_tokenizer(text):
    return text


# vectorizing training corpus with tfidf
tfidf = TfidfVectorizer(
    analyzer = 'word',
    tokenizer = id_tokenizer,
    preprocessor = id_tokenizer,
    token_pattern = None)

x_train_tf = tfidf.fit_transform(x_train)
x_test_tf = tfidf.transform(x_test)

#now for 1119 reviews in the training set, we have 1119 vectors of length 7288
print(x_train_tf.shape)

# # analyze vocabulary
# vocab_clean = tfidf.get_feature_names_out()
# print('The vocabulary contains ' + str(len(vocab_clean)) + ' tokens.')
# print('Some examples of these tokens:')
# print(vocab_clean[0:50])

"""The vocabulary includes some odd looking words with "-" and some all caps words. With only a-z chars the vocab is about 300 tokens less, but we decided to keep them for know and let the models figure out which tokens are important."""

best_model_results = []

"""### Logistic regression"""

param_grid_lr = {
    'penalty': ['l2', 'l1'],
    'C': [1, 5, 8, 10, 12],
    'solver': ['saga', 'liblinear'],
    'max_iter': [2000],
}

lr = LogisticRegression()

grid_search_lr = GridSearchCV(lr, param_grid_lr, cv=5, n_jobs=-1)
grid_search_lr.fit(x_train_tf, y_train)
lr = grid_search_lr.best_estimator_

print("Best hyperparameters: ", grid_search_lr.best_params_)
print("Best score: ", grid_search_lr.best_score_)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(lr.predict(x_test_tf), y_test)*100))

best_model_results.append({'name': 'Logistic Regression',
                           'best_gs_score': grid_search_lr.best_score_,
                           'test_accuracy': accuracy_score(lr.predict(x_test_tf), y_test)})

"""### SVM"""

param_grid_svc = {
        'C': [0.1, 0.5, 1, 2, 5, 10],
        'kernel': ['linear', 'rbf', 'poly'],
        'degree': [2, 3, 4],
        'gamma': ['scale', 'auto']
}

svc = SVC()
grid_search_svc = GridSearchCV(svc, param_grid_svc, cv=5, n_jobs=-1)
grid_search_svc.fit(x_train_tf, y_train)

svc = SVC(**grid_search_svc.best_params_)
svc.fit(x_train_tf, y_train)

print("Best hyperparameters: ", grid_search_svc.best_params_)
print("Best score: ", grid_search_svc.best_score_)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(svc.predict(x_test_tf), y_test)*100))

best_model_results.append({'name': 'SVM',
                           'best_gs_score': grid_search_svc.best_score_,
                           'test_accuracy': accuracy_score(svc.predict(x_test_tf), y_test)})

"""### Random Forest"""

param_grid_rf = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)

grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, n_jobs=-1)
grid_search_rf.fit(x_train_tf, y_train)
rf = grid_search_rf.best_estimator_

print("Best hyperparameters: ", grid_search_rf.best_params_)
print("Best score: ", grid_search_rf.best_score_)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(rf.predict(x_test_tf), y_test)*100))

best_model_results.append({'name': 'Random Forest',
                           'best_gs_score': grid_search_rf.best_score_,
                           'test_accuracy': accuracy_score(rf.predict(x_test_tf), y_test)})

"""### Naive Bayes"""

nb = MultinomialNB()
param_grid_nb = {
    'alpha': [0.1, 0.5, 1, 2, 5, 10],
    'fit_prior': [True, False]
}

grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1)
grid_search_nb.fit(x_train_tf, y_train)

nb = MultinomialNB(**grid_search_nb.best_params_)
nb.fit(x_train_tf, y_train)

print("Best hyperparameters: ", grid_search_nb.best_params_)
print("Best score: ", grid_search_nb.best_score_)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(nb.predict(x_test_tf), y_test)*100))

best_model_results.append({'name': 'Naive Bayes',
                           'best_gs_score': grid_search_nb.best_score_,
                           'test_accuracy': accuracy_score(nb.predict(x_test_tf), y_test)})

"""### LightGBM"""

param_grid_lgbm = {
        'num_leaves': [32, 64, 128],
        'max_depth': [3, 4, 5],
        'n_estimators': [50, 100],
        'learning_rate': [0.01, 0.05],
        'min_child_samples': [10, 20],
        'colsample_bytree': [0.5, 0.7],
        'reg_alpha': [0, 0.1],
        'reg_lambda': [0, 0.1]
}

lgbm = LGBMClassifier(random_state=42)

grid_search_lgbm = GridSearchCV(lgbm, param_grid_lgbm, cv=5, n_jobs=-1)
grid_search_lgbm.fit(x_train_tf, y_train)
lgbm = grid_search_lgbm.best_estimator_

print("Best hyperparameters: ", grid_search_lgbm.best_params_)
print("Best score: ", grid_search_lgbm.best_score_)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(lgbm.predict(x_test_tf), y_test)*100))

best_model_results.append({'name': 'LightGBM',
                           'best_gs_score': grid_search_lgbm.best_score_,
                           'test_accuracy': accuracy_score(lgbm.predict(x_test_tf), y_test)})

print("Best hyperparameters:  {'colsample_bytree': 0.5, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_samples': 10, 'n_estimators': 100, 'num_leaves': 32, 'reg_alpha': 0.1, 'reg_lambda': 0.1}")
print("Best score:  0.7337604099935937")
print("Test Accuracy  : 72.50 %")

"""### Stacking Ensemble"""

# create the base estimators
estimators = [('nb', nb), ('svc', svc), ('lr', lr)]

# create the stacking classifier
stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())

stacking_clf.fit(x_train_tf, y_train)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(stacking_clf.predict(x_test_tf), y_test)*100))

"""### Results"""

pd.DataFrame(best_model_results).sort_values(by='test_accuracy', ascending=False)

"""Confusion Matrix for Stacking Ensemble (best model in terms of accuracy)."""

# plot confusion matrix of best performing model in terms of accuracy
pred_labels = stacking_clf.predict(x_test_tf)

cm = confusion_matrix(y_test, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=le.inverse_transform([0,1,2,3]))

disp.plot()
plt.xticks(rotation=45)
plt.title("Confusion Matrix of Stacking Ensemble")
plt.show()

"""For comparison, confusion matrix of best accuracy logistic regression."""

# plot confusion matrix of best performing model in terms of accuracy
pred_labels = lr.predict(x_test_tf)

cm = confusion_matrix(y_test, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=le.inverse_transform([0,1,2,3]))

disp.plot()
plt.xticks(rotation=45)
plt.title("Confusion Matrix of best Logistic Regression")
plt.show()

"""## DistilBERT

Note: we use max_length=300 or 400 here. This means some reviews will be truncated, but it doesn't seem to be an issue for the model accuracy. With higher max_length like 512 (max for BERT) the errors during training occured.
"""

checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# apply DistilBERT tokenizer on all soft cleaned reviews
encoded_corpus = tokenizer(
    text=data["review_soft_clean"].tolist(),
    add_special_tokens=True,
    padding="max_length",
    truncation="longest_first",
    max_length=400,
    return_attention_mask=True,
    return_tensors="tf",
)

input_ids = encoded_corpus["input_ids"]
attention_mask = encoded_corpus["attention_mask"]

le = LabelEncoder()
labels = le.fit_transform(data["sentiment"])
labels = labels.reshape(-1, 1)

"""### DistilBERT as Feature Extractor"""

# put complete dataset through pretrained DistilBERT to get features
# from last hidden state of model

feature_model = TFDistilBertModel.from_pretrained(checkpoint)

dataset = Dataset.from_dict({"input_ids": input_ids, "attention_mask": attention_mask})

dataset = dataset.to_tf_dataset(
    columns=["attention_mask", "input_ids"],
    batch_size=32,
)

features = feature_model.predict(dataset, verbose=0)["last_hidden_state"][:, 0, :]

# DistilBERT embedding of all soft cleaned reviews
# for every review we have a 768-dimensional vector (embedding)
features.shape

x_train = features[train_index]
x_test = features[test_index]
y_train = data["sentiment"].values[train_index]
y_test = data["sentiment"].values[test_index]

"""#### Logistic Regression on DistilBERT features"""

param_grid_logreg = {
    'penalty': ['l2'],
    'C': [1, 5, 8, 10, 12],
    'max_iter': [1000],
}

logreg = LogisticRegression()

grid_search_logreg = GridSearchCV(logreg, param_grid_logreg, cv=5, n_jobs=-1)
grid_search_logreg.fit(x_train, y_train)
logreg = grid_search_logreg.best_estimator_

print("Best hyperparameters: ", grid_search_logreg.best_params_)
print("Best score: ", grid_search_logreg.best_score_)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(logreg.predict(x_test), y_test)*100))

"""#### SVM on DistilBERT features"""

param_grid_svc2 = {
        'C': [0.1, 0.5, 1, 2, 5, 10],
        'kernel': ['linear', 'rbf', 'poly'],
        'degree': [2, 3, 4],
        'gamma': ['scale', 'auto']
}

svc2 = SVC()
grid_search_svc2 = GridSearchCV(svc2, param_grid_svc2, cv=5, n_jobs=-1)
grid_search_svc2.fit(x_train, y_train)
svc2 = grid_search_svc2.best_estimator_

print("Best hyperparameters: ", grid_search_svc2.best_params_)
print("Best score: ", grid_search_svc2.best_score_)
print("Test Accuracy  : {:.2f} %".format(accuracy_score(svc2.predict(x_test), y_test)*100))

"""Worse performance than with TFIDF on untuned DistilBERT embeddings. Next try fine tuning on train data.

### DistilBERT Fine Tuning
"""

train_index2, val_index, _, _ = train_test_split(
    train_index,
    y_train,
    test_size=0.2,
    random_state=42,
    stratify=y_train
)

dataset_train = Dataset.from_dict(
    {
        "input_ids": tf.convert_to_tensor(input_ids.numpy()[train_index2]),
        "attention_mask": tf.convert_to_tensor(attention_mask.numpy()[train_index2]),
        "labels": labels[train_index2],
    }
)

dataset_val = Dataset.from_dict(
    {
        "input_ids": tf.convert_to_tensor(input_ids.numpy()[val_index]),
        "attention_mask": tf.convert_to_tensor(attention_mask.numpy()[val_index]),
        "labels": labels[val_index],
    }
)

dataset_test = Dataset.from_dict(
    {
        "input_ids": tf.convert_to_tensor(input_ids.numpy()[test_index]),
        "attention_mask": tf.convert_to_tensor(attention_mask.numpy()[test_index]),
        "labels": labels[test_index],
    }
)


dataset_train = dataset_train.to_tf_dataset(
    columns=["attention_mask", "input_ids"],
    label_cols="labels",
    shuffle=False,
    batch_size=32,
)

dataset_val = dataset_val.to_tf_dataset(
    columns=["attention_mask", "input_ids"],
    label_cols="labels",
    shuffle=False,
    batch_size=32,
)

dataset_test = dataset_test.to_tf_dataset(
    columns=["attention_mask", "input_ids"],
    label_cols="labels",
    shuffle=False,
    batch_size=32,
)

distilbert = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=4)

opt = keras.optimizers.Adam(learning_rate=1e-5)
loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

distilbert.compile(optimizer=opt, loss=loss, metrics=["accuracy"])

es = tf.keras.callbacks.EarlyStopping(monitor = "val_accuracy", mode="max", patience=10, restore_best_weights=True)
distilbert.fit(dataset_train, validation_data=dataset_val, epochs=100, callbacks=[es])

print("Best validation set performance:")
distilbert.evaluate(dataset_val, return_dict=True, batch_size=32)

print("Hold out test set performance:")
distilbert.evaluate(dataset_test, return_dict=True, batch_size=32)

# plot confusion matrix of best performing model in terms of accuracy
pred = distilbert.predict(dataset_test)
pred_labels = le.inverse_transform(np.argmax(pred.logits, axis=1))

cm = confusion_matrix(y_test, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=le.inverse_transform([0,1,2,3]))

disp.plot()
plt.xticks(rotation=45)
plt.show()

df_test = data.iloc[test_index].copy()
df_test['prediction'] = pred_labels
missclass = df_test[df_test['sentiment'] != df_test['prediction']]

for i, r in missclass.sort_values(by='sentiment').iterrows():
  print("True label:", r['sentiment'])
  print("Predicted label:", r['prediction'])
  print("Review:", r['review'])
  print("")

"""# Apply Best Model to the Real Test Set"""

test = pd.read_csv("/content/drive/MyDrive/test_just_reviews.txt", sep='\t', names = ['review'])

test["review_soft_clean"] = test["review"].apply(soft_clean_text)

encoded_test = tokenizer(
    text=test["review_soft_clean"].tolist(),
    add_special_tokens=True,
    padding="max_length",
    truncation="longest_first",
    max_length=400,
    return_attention_mask=True,
    return_tensors="tf",
)

dataset_test = Dataset.from_dict(
    {
        "input_ids": tf.convert_to_tensor(encoded_test["input_ids"].numpy()),
        "attention_mask": tf.convert_to_tensor(encoded_test["attention_mask"].numpy()),
    }
)

dataset_test = dataset_test.to_tf_dataset(
    columns=["attention_mask", "input_ids"],
    shuffle=False,
    batch_size=32,
)

pred = distilbert.predict(dataset_test)

# write labels to results.txt file
pred_labels = le.inverse_transform(np.argmax(pred.logits, axis=1))
with open(r'/content/drive/MyDrive/results.txt', 'w') as fp:
    for label in pred_labels:
        fp.write("%s\n" % label)